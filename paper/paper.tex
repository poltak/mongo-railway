\documentclass[a4paper,11pt]{article}

\usepackage[paper=a4paper,left=30mm,width=150mm,top=25mm,bottom=25mm]{geometry}
\usepackage[hidelinks]{hyperref}    % All references will auto hyperlink
\usepackage[font={small}]{caption}  % Make figure captions smaller
\usepackage{graphicx}               % For the displaying of images in floats
\usepackage{mathtools}              % For more advanced math usage
\usepackage{poltakmacros}           % Personal macro package (https://gist.github.com/poltak/6ed59e8f30c9c399143f)
\usepackage{listings}
\usepackage{color}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, delete, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=left,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}


\begin{document}

\include{titlepage}                 % Pull in title page

\newpage
\pagenumbering{roman}
\tableofcontents
\newpage

\pagenumbering{arabic}


\section{Introduction} % (fold)
\label{sec:introduction}

While relational database management systems (RDBMS) have been somewhat of a ``go-to'' solution for for a number of years
for general data storage and management problems that many application developers face, we have noted a recent rise
in the use of non-relational data management tools~\cite{padhy2011rdbms}. Most of these tools have traditionally fallen into the domain of
big data analytics, with platforms such as the Hadoop ecosystem~\footnote{https://hadoop.apache.org/} being notably popular.
However, outside of the domain of big data, looking more at general purpose data storage and management, what are now
commonly referred to as ``NoSQL'' solutions are proving to be a popular solution.

NoSQL databases refer to those databases that are not built on top of the relational algebraic concepts, as laid out by
Codd in 1970~\cite{codd1970relational}, unlike the more commonly used RDBMS technologies, such as
MySQL~\footnote{https://www.mysql.com/}. Being free of the strictness the relational model enforces on its data allows
NoSQL databases to focus less on the overall structure of data, and more on factors such as scalability and
performance~\cite{leavitt2010will}.

While the relational model is a good fit for many data problems, its strictness in terms of flexibility of managing data
eventually led to the introduction of the NoSQL model. The following characteristics can be given as a starting point
for NoSQL databases in comparison to relational databases~\cite{indrawan2012database}:

\begin{itemize}
  \item \textbf{Unstructured data support:} While the relational model would often force data to be stored in tabular
  formats, the NoSQL model does not force any kind of data schema.
  \item \textbf{Designed with distributed processing and horizontal scalability in mind:} Given the commoditisation of
  computer hardware in the last decade, support for horizontal scaling and processing among clusters is an important
  factor for adoption.
  \item \textbf{Less strict adherence to ACID principles:} While the relational model attempted to very much adhere to
  the transactional principles of data atomicity, consistency, isolation, and durability (ACID), this very much impacts
  performance in terms of distributed computing. Relaxing the strictness of adherence to these principles, allow many
  NoSQL databases to make the trade-off for higher performance.
\end{itemize}

Of course, these differences between the NoSQL model and relational model vary between each individual database
technology's design, and trade-offs are often made depending on the goals and aims for that given database.

In this paper, we will look at the use of MongoDB~\footnote{https://www.mongodb.org/}, a popular NoSQL database solution,
as a solution for a case study based upon a railway data problem using data from Monash University's Institute of Railway
Technology (IRT). An overview of the case study in question will be given in~\sectref{sec:case_study_overview}. A small
overview of the MongoDB database will be given in~\sectref{sec:mongo_overview}. Implementation and
discussion details will be given in~\sectref{sec:implementation} and~\sectref{sec:discussion}, respectively,
before concluding in~\sectref{sec:conclusion}.

% section introduction (end)

\newpage

\section{Case Study Overview} % (fold)
\label{sec:case_study_overview}

The case study that is being looked at involves a project that has been worked at at the Monash University Institute of
Railway technology (IRT). The project involves trains the operate in the Pilbara region of Western Australia, taking
ore and minerals from loading points at mines to a specified unloading port. The data that is being dealt with comes from
numerous categories of sensors being placed on certain specialised train cars to record data monitoring track and train
car conditions~\cite{darby2003development,darby2005track}.

Data currently gets unloaded at sent back in large batches to remote servers once the car completes a trip and pulls back
into port~\cite{thomas2012taking}. The project currently makes use of a RDBMS solution to manage data storage, however
this solution is faced with many problems that currently require painful work-arounds. The most obvious of which involves
unreliable data being received from sensors. For example, in the case of a damaged or failed sensor, reliable data cannot
be guaranteed to be returned from such a sensor. Hence, data received by the remote server is often inconsistently
structured, and thus the data has to go through a series of preprocessing work arounds to fit in within the strict
schema that the RDBMS expects. As the only guaranteed consistency in the data that gets received is the timestamp and
geocoordinates, the strictly structured relational model is not an appropriate solution.

The Monash IRT team are currently investigating further solutions in the NoSQL big data space, where they intend to
replace the current system with an appropriately tested solution. While the scope of that project is much larger than
what will be covered in this paper, we will look at the differences in what is possible when attempting to use a NoSQL
database for the data storage and management.

For this paper, we will be looking at the possibility of using MongoDB, a popular NoSQL database that does away with
the concept of the schemas and tables so commonly found in databases following the relational model. This is done with
the expectation of better handling of the given inconsistent data.

% section case_study_overview (end)

\newpage

\section{MongoDB Overview} % (fold)
\label{sec:mongo_overview}

\textit{Note that most of the information relevant to MongoDB in this section is sourced from the official MongoDB
manual.}~\cite{Mongo_man}

\subsection{Data storage} % (fold)
\label{sub:data_storage}

MongoDB can be described as a documented-oriented NoSQL database, which does not rely on schemas or tables to structure
its data~\cite{parker2013comparing}. Data within Mongo is stored using JSON-like structured documents, called
BSON~\cite{bsons6:online}, which allows for strongly typed data fields and binary data. BSON documents are then stored
within ``collections'' which reside in a given ``database''~\cite{parker2013comparing}. To relate back to the relational
model, these concepts within Mongo can be compared as shown in~\tabref{tab:mongo_to_relational}.

\begin{table}[h]
\centering
\caption{MongoDB conceptual structures as they relate to the relational model concepts.}
\label{tab:mongo_to_relational}
\begin{tabular}{ | l | l | }

\hline
\textbf{MongoDB}  & \textbf{Relational Model} \\ \hline
Database          & Database                  \\ \hline
Collection        & Table                     \\ \hline
BSON Document     & Record/Row/Tuple          \\ \hline
BSON Key          & Column/Field              \\ \hline
BSON Value        & Value                     \\ \hline

\end{tabular}
\end{table}

While these concepts within Mongo can be related back to concepts within the relational model for ease of understanding,
they are not exactly the same. For example, a table in the relational model is structured in such a way that all records
within that table must adhere to. Conversely, in a Mongo collection, several differently structured BSON documents may
coexist freely. BSON documents generally follow some sort of predefined structure for that collection, to maintain
data structure sanity, for the user's sake, however none of this is enforced.

This sort of freedom in terms of structure allows for similarly structured documents in database collections, where
differences may only be found in that certain documents may miss keys that are present in other documents. This is
highly rational in the way that, depending on the use-case, data may not be present for certain fields all the time.
Thus instead of placing null or default values for missing data, Mongo allows the user to just leave them out of the
relevant document. Note that document structure in Mongo is also dynamic in the way that it can be changed at any time;
BSON keys and values may be added or removed from existing documents at any time.

% subsection data_storage (end)

\subsection{Data interaction} % (fold)
\label{sub:data_interaction}

MongoDB allows a lot of the similar type functionality in queries and data manipulation that is also expected in
relational databases. Generally interactions with data are performed through JavaScript functions on collections, or cursors
(subsets of documents within a collection), allowing general querying operations through use of the \texttt{find()} function,
and standard CRUD operations through the \texttt{create()}, \texttt{update()}, and \texttt{remove()} functions.

MongoDB officially provides support for many drivers allowing interaction with Mongo databases in different programming
languages, including JavaScript, Python, and Java. As well as providing drivers, MongoDB also provides a default shell,
called the \textit{mongo shell}, which allows general JavaScript code to be written to interact with the standard MongoDB
API and perform ad-hoc queries. No official graphical user interface exists to interact with MongoDB, however there
are numerous third party clients, which essentially wrap around mongo shell.

MongoDB also supports more advanced data manipulation operations through use of aggregation functions. Generally, aggregation
functions are user defined operations to perform on documents in collections to return results. These include general
aggregate operation models, such as MapReduce, and often involve grouping documents by given keys and various queries
to limit the amount of documents to process.


% subsection data_interaction (end)

\subsection{Further database features} % (fold)
\label{sub:further_database_features}

\subsubsection{Lack of joins} % (fold)
\label{ssub:joins}

MongoDB, like many other NoSQL databases, does not support the joins of multiple tables or, in MongoDB's case, documents,
as is common in relational databases.
Instead, MongoDB allows two main ways of relating different documents to each other. These are via referencing and
nested documents. Nested documents are the main recommended way of join multiple documents, unless the documents have
many-to-many relationships. To allow nested documents, a BSON value can store a whole other document, or arrays of documents.
Example use-cases of where nested documents are appropriate include documents that have natural ``child'' documents that
only it will ever need to reference. For example, a blog post document may have many nested comment documents as the
comments are specific to a given blog post. For documents where many things may refer to it, it is recommended to use
references via BSON values containing other document IDs.

% subsubsection joins (end)

\subsubsection{Indexing} % (fold)
\label{ssub:indexing}

Indexing in MongoDB is supported in a similar ways that are present in relational databases. Mongo builds a B-Tree data
structure on a particular collection's key, allowing all documents within that collection with that key to be accessed
in $O(log n)$ time, as opposed to $O(n)$ time if unindexed. MongoDB also supports numerous types of indexes, such as
single-key, multi-key, geospatial, and hashed indexes.

% subsubsection indexing (end)

\subsubsection{Replication} % (fold)
\label{ssub:replication}

MongoDB allows replication and management of multiple copies of the same data for the purpose of increasing data
redundancy and availability. Replication in Mongo is managed using a primary instance of data, with secondaries replicating
the data in the primary using operation logs (\textit{oplogs}) to mimic any operations on data that were performed in the
primary. Replicas have fault tolerance in the way that if the primary fails, an election is started to promote any of the
secondaries to become a primary. In general, a client will only interact with a primary, however operations can also be
specified to be performed on specific secondaries. However, as replication data is asynchronously managed, any reads
performed specifically on secondaries may not give up-to-date data.

% subsubsection replication (end)

\subsubsection{Sharding} % (fold)
\label{ssub:sharding}

Sharding in MongoDB allows for horizontal scaling of a Mongo database, where a given database or collection is shared
among different ``shards'', or multiple MongoDB instances, generally running on separate machines. Sharding is performed
on an indexed document key, dividing documents containing that key based on either range or computed hashes. Ranged-based
sharding is generally recommended where data is generally expected to be accessed in some sort of sequence, while hash-based
sharding offers a guaranteed random distribution of data across shards, and more balanced equally-sized shards.

% subsubsection sharding (end)


% subsection further_database_features (end)

% section mongo_overview (end)


\newpage

\section{Implementation} % (fold)
\label{sec:implementation}

Implementation of the database for the given Monash IRT data was all performed using MongoDB version 3.0.3, the latest
version as of 2015-05-18. Interaction with the MongoDB database was all performed using the mongo shell and corresponding
official NodeJS MongoDB driver.

\subsection{Overview of the data} % (fold)
\label{sub:overview_of_the_data}

The test data used for this case study was acquired from the Monash IRT team, and was an extract of real data they have
received from the railway sensors. As only one set of data was given, simulated datasets based upon the extract have
also been created to simulate the type of inconsistencies the IRT team are facing, including differently structured
datasets and datasets with missing data (in the case of non-functional sensors).

The dataset received from the IRT team has the layout as shown in~\tabref{tab:irt_data}. It consists of 99\,999 data points
recorded at different times, and is received as raw data before being processed into a CSV format for easy manipulation
and subsequent storage into their database.

\begin{table}[h]
\caption{The different types of data received from the Monash University Institute of Railway Technology team.}
\label{tab:irt_data}
\tabcolsep=0.11cm
\scalebox{0.7}{
\begin{tabular}{ | l | l | l | }

\hline
\textbf{Name}           & \textbf{Data type}                     & \textbf{Description} \\ \hline
\textbf{Time}           & Floating point value                   & Time in seconds since the start of the test. \\ \hline
\textbf{Speed}          & Floating point value                   & Speed in kilometres per hour. \\ \hline
\textbf{LoadEmpty}      & -1, 0, 1                               & 1 if load is present, 0 if empty, or -1 if unknown/in-port.  \\ \hline
\textbf{km}             & Floating point value                   & Elapsed kilometres travelled since the start of the test.\\ \hline
\textbf{Lat}            & Floating point value from -90 to +90   & Geographic latitude coordinate at given reading.  \\ \hline
\textbf{Lon}            & Floating point value from -180 to +180 & Geographic longitude coordinate at given reading.  \\ \hline
\textbf{Track}          & Integer value from 1 to 213\,841         & Given track in which the train is located.   \\ \hline
\textbf{SND1}           & Floating point value                   & Spring nest deflection count in millimetres at car corner.  \\ \hline
\textbf{SND2}           & Floating point value                   & Spring nest deflection count in millimetres at car corner.  \\ \hline
\textbf{SND3}           & Floating point value                   & Spring nest deflection count in millimetres at car corner.  \\ \hline
\textbf{SND4}           & Floating point value                   & Spring nest deflection count in millimetres at car corner.  \\ \hline
\textbf{CouplerForce}   & Floating point value                   & The amount of force couple detected in the train car.  \\ \hline
\textbf{LateralAccel}   & Floating point value                   & The amount of lateral acceleration detected in the train car.  \\ \hline
\textbf{AccLeft}        & Floating point value                   & The amount of acceleration detected by the left sensor.  \\ \hline
\textbf{AccRight}       & Floating point value                   & The amount of acceleration detected by the right sensor.  \\ \hline
\textbf{BounceFront}    & Floating point value                   & The amount of bounce detected at the front of the car.  \\ \hline
\textbf{BounceRear}     & Floating point value                   & The amount of bounce detected at the rear of the car.  \\ \hline
\textbf{RockFront}      & Floating point value                   & The amount of rock detected at the front of the car.  \\ \hline
\textbf{RockRear}       & Floating point value                   & The amount of rock detected at the front of the car.  \\ \hline

\end{tabular}}
\end{table}

This dataset contains data from all the possible sensors. As previously stated, the datasets received are not always
consistent, hence not all of these fields will always be present, or may be present and contain incorrect data readings.
The only sensor data guaranteed to be received is the \textbf{Time}, \textbf{Lat}, and \textbf{Lon} sensor recordings.

% subsection overview_of_the_data (end)


\subsection{Overview of data handling} % (fold)
\label{sub:overview_of_data_handling}

None of the data returned from the sensors contain overly complicated structures, and hence MongoDB's BSON data storage
format can easily handle all of the given data types. All given types can be handled using the BSON primitive types of
Double and 32-bit Integer for floating point and integer data, respectively.

MongoDB comes with a built-in tool, called \texttt{mongoimport}, for the importing of multiple common data files, including
CSV. Hence, the IRT data can be imported into a running MongoDB instance using the following command pattern:

\begin{lstlisting}[language=bash]
mongoimport -d DATABASE_NAME -c COLLECTION_NAME --type csv --file FILE_NAME.csv --headerline --ignoreBlanks
\end{lstlisting}

The \texttt{-d} and \texttt{-c} flags enable the specifying of database and collection within that database in which to
import the data into. The \texttt{--headerline} flag asks Mongo to use the CSV header to automatically generate BSON
keys for each document. So each row in the CSV file will be converted to a MongoDB document, and stored in the specified
database collection. By default, MongoDB tries to guess the data type for each value, with numbers represented as Doubles.
Of course, storing every document as a flat container for different floating point values is not optimal, and pre-processing
can be performed on Mongo to better structure the documents for later querying and processing.

% subsection overview_of_data_handling (end)


\subsection{Updating of data types} % (fold)
\label{sub:restructuring_of_data}

One of the first operations needed to be performed to restructure the data within MongoDB is to assign better data types
fields where it is appropriate. With the given sensor data fields, the most obvious would be the \textbf{Track} and
\textbf{LoadEmpty} fields, as the possible values they should be able to store are limited. With the \textbf{Track} field,
the only values that should be present are integers representing the current track that the train car is located. Currently,
the track numbers in the scope of the IRT project range from 1 to 213\,841. These all fall into the 32-bit integer representation
that BSON supports, hence rather than the default Double type, it more be more appropriate to change it to the 32-bit Integer
type. This can be done iteratively with the following code:

\begin{lstlisting}
var query = {Track: {$exists: true}};

db.train.find(query).forEach(function(doc) {

  // Cast Track to be Int32 type
  doc.Track = new NumberInt(doc.Track);

  // Write changes to MongoDB
  db.train.save(doc);

});
\end{lstlisting}

With the \textbf{LoadEmpty} field, the only possible values are -1, 0, and 1, each representing different states. While
this would be a perfect case to use an enumerated type, MongoDB does not natively support enums. The two main choices
for handling enums is by using either the String or 32-bit Integer types. As the String type allows more expressive
values, and can be indexed to have integer-like performance, it would be most appropriate to change the field to be
represented as a String type, mapping the existing numbers to appropriate strings. As the existing \textbf{LoadEmpty}
value will need to be changed, this operation can be performed with the \texttt{db.collection.update()} function, as
follows:

\begin{lstlisting}
// Change LoadEmpty = 1 to 'loaded'
db.train.update(

  // query
  { "LoadEmpty" : 1 },

  // update
  { $set: {LoadEmpty: 'loaded'} },

  // options
  {
    "multi" : true,   // Update all documents
    "upsert" : false  // Insert a new document, if no existing document match the query
  }
);

// Change LoadEmpty = 0 to 'empty'
db.train.update(

  // query
  { "LoadEmpty" : 0 },

  // update
  { $set: {LoadEmpty: 'empty'} },

  // options
  {
    "multi" : true,   // Update all documents
    "upsert" : false  // Insert a new document, if no existing document match the query
  }
);

// Change LoadEmpty = -1 to 'unknown'
db.train.update(

  // query
  { "LoadEmpty" : -1 },

  // update
  { $set: {LoadEmpty: 'unknown'} },

  // options
  {
    "multi" : true,   // Update all documents
    "upsert" : false  // Insert a new document, if no existing document match the query
  }
);
\end{lstlisting}

Note that three different updates need to be performed on the collection, for each different value of \textbf{LoadEmpty},
as filtered out by the query argument. As each update will need to query the collection to retrieve a subset of documents
to operate on, it would be wise to create a MongoDB index on the \textbf{LoadEmpty} key. This single-key index can be
created using the following \texttt{db.collection.createIndex()} function:

\begin{lstlisting}
// Create an ascending index on LoadEmpty field
db.train.createIndex({LoadEmpty: 1});
\end{lstlisting}

% subsection restructuring_of_data (end)


\subsection{Restructuring of documents} % (fold)
\label{sub:restructuring_of_documents}

As the documents now contain appropriately typed key-value pairs, further modifications to the documents can be performed
to better structure the documents. By default, the CSV importer structures the documents generated from CSV rows as flat
documents, with all row values being represented as a discrete key-value pair. The default flat structure can be seen as
follows:

\begin{lstlisting}
{
  "_id" : ObjectId("5561e94530f853aef8bbfd0c"),
  "Time" : 1094468487.0999999046325684,
  "Speed" : 4.4783997535999998,
  "LoadEmpty" : -1.0000000000000000,
  "km" : 5.0096998214999999,
  "Lat" : -20.6665750019999983,
  "Lon" : 117.1280816499999986,
  "Track" : 2.0000000000000000,
  "SND1" : 0.0118295024810000,
  "SND2" : -0.0173957574360000,
  "SND3" : -0.0295975576070000,
  "SND4" : -0.0327308348410000,
  "CouplerForce" : 23.1114991600000010,
  "LateralAccel" : 0.0033679292537000,
  "AccLeft" : 0.0000000000000000,
  "AccRight" : 0.0000000000000000,
  "BounceFront" : -0.0104506661800000,
  "BounceRear" : -0.0234966575210000,
  "RockFront" : 0.0445603373210000,
  "RockRear" : 0.0122018001710000
}
\end{lstlisting}

From looking at and understanding the
sensor data that is being dealt with, numerous sensors can be separated into distinct groups. For example, each of the
\textbf{SND1}, \textbf{SND2}, \textbf{SND3}, and \textbf{SND4} values are from the same type of sensor located at different
parts of the train car. Hence, rather than having four discrete key-pairs, these values can be grouped together in their
own sub-document, or object, and pointed to by a new \textbf{SND} key. This is a more sane and appropriate way of
structuring these values for later queries and operations.

Numerous other sensor values can be grouped together in this manner for more appropriate and saner structuring using
sub-documents within the original BSON documents. Often nested documents are used where separate tables would be used in
a relational modelling of the dataset, which also stretches to this case, where a separate table could possible hold all
SND sensor values.

Further restructuring can be done to group the \textbf{Bounce}, \textbf{Rock}, and \textbf{Acc} values into sub-documents
containing key-value pairs for each, finally ending up with the following document structure:

\begin{lstlisting}
{
  "_id" : ObjectId("555d30ca33d9d7607772920f"),
  "Time" : 1094468487.0999999046325684,
  "Speed" : 4.4783997535999998,
  "LoadEmpty" : "unknown",
  "km" : 5.0096998214999999,
  "Track" : 2,
  "CouplerForce" : 23.1114991600000010,
  "LateralAccel" : 0.0033679292537000,
  "SND" : {
    "SND1" : 0.0118295024810000,
    "SND2" : -0.0173957574360000,
    "SND3" : -0.0295975576070000,
    "SND4" : -0.0327308348410000
  },
  "Loc" : {
    "Lat" : -20.6665750019999983,
    "Lon" : 117.1280816499999986
  },
  "Bounce" : {
    "Front" : -0.0104506661800000,
    "Rear" : -0.0234966575210000
  },
  "Rock" : {
    "Front" : 0.0445603373210000,
    "Rear" : 0.0122018001710000
  },
  "Acc" : {
    "Left" : 0.0000000000000000,
    "Right" : 0.0000000000000000
  }
}
\end{lstlisting}

These restructurings could be performed with the following code, in mongo shell for example:

\begin{lstlisting}
var updateDocsFunc = function(doc) {
  // Add location nested doc
  doc.Loc = {
    Lat: doc.Lat,
    Lon: doc.Lon
  };

  // Add Bounce nested doc
  doc.Bounce = {
    Front: doc.BounceFront,
    Rear: doc.BounceRear
  };

  // Add Rock nested doc
  doc.Rock = {
    Front: doc.RockFront,
    Rear: doc.RockRear
  };

  // Add SND nested doc
  doc.SND = {
    SND1: doc.SND1,
    SND2: doc.SND2,
    SND3: doc.SND3,
    SND4: doc.SND4
  };

  // Add Acc nested doc
  doc.Acc = {
    Left: doc.AccLeft,
    Right: doc.AccRight
  };

  // Delete old values
  delete doc.Lat;
  delete doc.Lon;
  delete doc.BounceFront;
  delete doc.BounceRear;
  delete doc.RockFront;
  delete doc.RockRear;
  delete doc.SND1;
  delete doc.SND2;
  delete doc.SND3;
  delete doc.SND4;
  delete doc.AccLeft;
  delete doc.AccRight;

  // Write changes
  db.train.save(doc);
};

// Iteratively perform operations on all records
db.train.find().snapshot().forEach(updateDocsFunc);
\end{lstlisting}

Finally, one more restructure could be performed on the existing document structure, which is grouping the \textbf{Lat}
and \textbf{Lon} values into a single sub-document, which could be named \textbf{Loc}. This further enables future
geospatial indexes being more easily created on the location data, for location-dependent queries and operations.
This restructuring can be performed by the following code:

\begin{lstlisting}
// Create a geospatial index on Loc nested document
db.train.createIndex({Loc: '2d'});
\end{lstlisting}

% subsection restructuring_of_documents (end)

\subsection{Handling missing values} % (fold)
\label{sub:handling_missing_values}

For the test datasets missing data for particular sensors, MongoDB handles these cases. The datasets include
CSV files with different value layouts, and often missing values in random rows, apart from the time and location fields
as they are guaranteed.

For test datasets that are organised in different orders of fields, as Mongo's inner data storage format, BSON, does
not have any ordering of its keys, documents are still generated with the appropriate data. For datasets with missing
values in certain rows, the BSON document that is generated simply does not contain a key-value pair for the missed
attribute. This is opposed to setting the key to a null value, or default value. Of course, a user may generate keys
for the missing values and point them to null or default values, however there is no advantage to the extra processing.

When operations are performed on multiple documents in Mongo, if the documents contain a different amount of keys, Mongo
effectively ignores this and continues to perform the operation on all documents. Generally, specific keys and values
are passed to operations such as these, hence if the key used does not exist within certain documents, those documents
simply get ignored from the operation. For example, if a collection contains 100 documents, where only four of those
documents contain the \textbf{isAdmin} key and an operation is called that depends on the key \textbf{isAdmin}, only
those four documents within the collection will be operated on. This allows completely differently structured documents
to be contained within the same collection (however, in a well-structured dataset, multiple collections would be used).

Hence, in the IRT case study, if certain sensors break down during operation and are unable to send back data, MongoDB
will still allow queries and operations to be performed on the overall sensor data. However, if any operations depend on
a particular sensor's data, such as the operations performed in the pre-processing of the data
in~\sectref{sub:restructuring_of_data}, the documents missing those sensor's data will simply be left out of the operation. This
avoids errors that are currently encountered with the currently set default values for broken sensors. For instance, if
a sensor is broken and needs to be used in an operation, a lot less documents will be needed to be involved in the
query, increasing query performance. Furthermore, any reducing operations performed on the datasets, will not be subject
to involving unneeded default values in their calculations.

% subsection handling_missing_values (end)


\subsection{Filtering bad data} % (fold)
\label{sub:filtering_bad_data}

% subsection filtering_bad_data (end)



% section implementation (end)


\newpage

\section{Discussion} % (fold)
\label{sec:discussion}

% section evaluation_and_discussion (end)


\newpage

\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)

\newpage                            % Make references section

\bibliographystyle{acm}
\bibliography{biblio}

\end{document}
